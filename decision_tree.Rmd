---
title: "R Notebook"
output: html_notebook
Author: "Akshay Kale"
editor_options: 
  chunk_output_type: console
---
# Introduction
  The 2017 Infrastructure report card gives a C+ rating to Bridges in the US. Out of several recommendations to improve the health of the bridges, ASCE suggests that bridges owners should consider the cost across the bridgeâ€™s entire lifecycle to make smart design decisions and prioritize maintenances and rehabilitation. Several researchers provided method to evaluate the performance of the bridges over their period of time. The next step in addressing this problem is to identify the influential factors in the performance of the bridges and the maintenance of the bridges. However, there is a lack of general understanding what keeps bridges in better performning state and the effect of maintainace on the performance of the bridges.Why do we think there is no general understanding of the performance and the and maintenance? because, previous research didn't account the performance over their entire life-cycle of the bridges to predict the future maintenances and rehabilitation of the bridges. In this research study, we studied 600,000 bridges to understand and identify the most influential factors in performance of the bridge and the role of the maintenance in the performance of the bridge. We will examine the role of bridge attributes, environmental, and demographical factors in the performance and accounting the repair and rehabilitation of the bridges.


```{r}
library(lattice)
library(ISLR)
library(MASS)
library(caret)
library(tidyverse)
library(dplyr)
```

# Introduction
# Previous Research
# Data
# Method
     
# Steps:

  1. Transform data and create a dataset in python
  2. Create a random forest model to select most important features for 1) Predicting performance, 2) Predicting intervention
  3. Comparision of the two models
  
  
# Number of features to take into consideration:
  
  1. Structure Number (X)
  2. State Name (X)
  3. Age (X)
  4. Average Daily Traffic (X)
  5. Average Daily Truck Traffic (X)
  6. Superstructure (X)
  7. Average Daily Precipitation (X)
  8. Superstructure (Condition Rating) - Collinearity (Not completely independent) (X)
  9. Structure Type (X)
  10. Length of maximum span flat (X)
  11. Number of Snowfall days (X)
  12. Number of freeze Thaw (X)
  13. Structure Type (X)
  14. Owner or Maintainer (X)
  15. Length of Maximum Span Flat (X)
  16. Baseline Difference Score  (X)
  17. Number of Invtervention / Intervention (X)
  18. Population (X)
  19. Route Signing Prefix 
  20. Designated Level of Service
  21. Inventory Route, Minimum Vertical Clearance
  22. Base Highway Network
  23. Bypass, Detour Length
  24. Toll
  25. Functional Classification of Inventory Route  
  26. Year_L (X)
  27. Lanes On and Under the Structure 
  28. Traffic Safety Features  
  29. Structure Flared
  30. Skew
  31. Historical Significance
  






```{r}

# Reading the dataset
nbi <- read.csv('apple.csv')
names(nbi)

## Data processing
# Converting into factors
library(plyr)
nbi$Maintainer <- as.factor((nbi$Maintainer))
nbi$State.Code <- as.factor((nbi$State.Code))
nbi$Owner <- as.factor((nbi$Owner))
nbi$Material <- as.factor((nbi$Material))
nbi$Structure.Type <- as.factor((nbi$Structure.Type))
#nbi$Reconstruction_binary <- as.factor((nbi$Reconstruction_binary))
#nbi$Rehabilitation_binary <- as.factor((nbi$Rehabilitation_binary))
#nbi$Repair_binary <- as.factor(nbi$Repair_binary)

# Mapping names to the codes
nbi$Material <- mapvalues(nbi$Material, 
                from = c("1", "2", "3", "4", "5", "6", "7", "8", "9", "-1", "0"),
                 to = c("Concrete", "Concrete Continuous", "Steel", "Steel Continuous", "Prestressed Concrete","Prestressed  Concrete Continuous", "Wood or Timber", "Masonry", "Aluminium", "NA", "Other"))

nbi$Rehabilitation_binary <- mapvalues(nbi$Rehabilitation_binary, 
                             from = c("1","0"),
                          to = c("Yes", "No"))


nbi$Reconstruction_binary <- mapvalues(nbi$Reconstruction_binary, 
                             from = c("1","0"),
                             to = c("Yes", "No"))


nbi$Repair_binary <- mapvalues(nbi$Repair_binary, 
                             from = c("1","0"),
                             to = c("Yes", "No"))

```


#### Training
```{r}
library(doParallel)
num_cores <- detectCores() #note: you can specify a smaller number if you want
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)


# Creating training and testing set
target_variable <- 'Reconstruction_binary'

index = createDataPartition(y=nbi[[target_variable]], p=0.7, list=FALSE)
train.set = nbi[index,]
test.set = nbi[-index,]


# Tuning grid parameter
grid <- expand.grid(cp=seq(0.0,.01,0.001))
model <- Reconstruction_binary ~  Age_L + Material + ADT.Category + ADTT.Category + Maintainer + Avg..Daily.Precipitation + Structure.Type


# Training
nbi.tree = train(model,
                 data = train.set,
                 method = "rpart2",
                 trControl = trainControl(method = "repeatedcv", search = 'random', repeats = 5,    summaryFunction = twoClassSummary, classProbs = T, savePredictions = T),
                 tuneLength = 20,
                 metric='ROC')

suppressMessages(library(rattle))
library(rpart)
library(rpart.plot)

# Variable Importance
varImp(nbi.tree)

#View model
nbi.tree

# Plot Accuracy vs Complexity Paramenter
plot(nbi.tree)

# Plot tree
#fancyRpartPlot(nbi.tree$finalModel)

# Prediction on the test set
nbi.pred = predict(nbi.tree, newdata = test.set)

# 
table(nbi.pred, test.set$Category)
error.rate = round(mean(nbi.pred != test.set[[target_variable]]), 2)
error.rate

# Confusion Matrix
confusionMatrix(nbi.pred, test.set[[target_variable]])
```



# Plotting Tree
```{r}
library(rpart.plot)
rpart.plot(nbi.tree$finalModel)
```




#### Model for predicting the baseline difference score
```{r}
library(doParallel)
num_cores <- detectCores() #note: you can specify a smaller number if you want
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)


# Creating training and testing set
target_variable <- 'Category'

index = createDataPartition(y=nbi[[target_variable]], p=0.7, list=FALSE)
train.set = nbi[index,]
test.set = nbi[-index,]


# Tuning grid parameter
grid <- expand.grid(cp=seq(0.0,.01,0.001))
model <- Category ~ State.Code + Age_L + Material + ADT.Category + ADTT.Category + Maintainer + Avg..Daily.Precipitation + Structure.Type


# Training
nbi.tree = train(model,
                 data = train.set,
                 method = "rpart",
                 trControl = trainControl(method = "repeatedcv", search = 'random', repeats = 5),
                 tuneLength = 10, metric = "ROC")

suppressMessages(library(rattle))
library(rpart)
library(rpart.plot)

# Variable Importance
varImp(nbi.tree)

# View model
nbi.tree

# Plot Accuracy vs Complexity Paramenter
plot(nbi.tree)

# Plot tree
#fancyRpartPlot(nbi.tree$finalModel)

# Prediction on the test set
nbi.pred = predict(nbi.tree, newdata = test.set)

# 
table(nbi.pred, test.set$Category)
error.rate = round(mean(nbi.pred != test.set[[target_variable]]), 2)
error.rate

# Confusion Matrix
confusionMatrix(nbi.pred, test.set[[target_variable]])
```
# include ROC, 
# undersampling
# threshold value 
# tunelength 


```{r}
rpart.rules(nbi.tree$finalModel)
```

